[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mathematical Methods for Biology, Part 2",
    "section": "",
    "text": "Preface\nIn this book you will find a collection of mathematical ideas, computational methods, and modeling tools for describing biological systems quantitatively. Biological science, like all natural sciences, is driven by experimental results. As with other sciences, there comes a point when accumulated data needs to be analyzed quantitatively, in order to formulate and test explanatory hypotheses. Biology has reached this stage, thanks to an explosion of data from molecular biology techniques, such as large-scale DNA sequencing, protein structure determination, data on gene regulatory networks, and signaling pathways. Quantitative skills have become necessary for anyone hoping to make sense of biological research.\nMathematical modeling necessarily involves making simplifying assumptions. Reality is generally too complex to be captured in a few equations, and this is especially true for living systems. Simplicity in modeling has at least two virtues: first, simple models can be grasped by our limited minds, and second, it allows for meaningful testing of the assumptions against the evidence. A complex model that fits the data may not provide any insights about how the system works, whereas a simple model which does not fit all the data can indicate where the assumptions break down. We will learn how to construct progressively more sophisticated models, beginning with the ridiculously simple."
  },
  {
    "objectID": "index.html#modeling-assumptions-theoretical-and-empirical",
    "href": "index.html#modeling-assumptions-theoretical-and-empirical",
    "title": "Mathematical Methods for Biology, Part 2",
    "section": "modeling assumptions: theoretical and empirical",
    "text": "modeling assumptions: theoretical and empirical\nA mathematical model postulates a precise relationship between several quantities, attempting to mimic the behavior of a real system. All models rest on a set of assumptions, postulating how various quantities are interrelated. These assumptions generally come from two sources: a scientific theory, or experimental observations. For instance, a model of molecular motion may rest on the assumption that Newton’s laws hold true. On the other hand, the observation that a drug injected into the bloodstream of a mammal is metabolized with an exponential time dependence is empirical. The benefit of models based on well-established theories, sometimes known as “first-principles models”, is that they can be constructed without prior experimental knowledge of a particular system. Newton’s laws apply to all sorts of classical mechanics objects, ranging in size from molecules to planets. Some prefer first-principles models, because they rely on well-established scientific principles, while others will argue that an empirical model more accurately reflects the behavior of the system at hand. From a mathematical standpoint, there is no difference between the two types of models. We will use the same tools to construct and analyze models, regardless of their origin.\nA stated assumption can be written as a mathematical relationship, usually in the form of an equation relating quantities of interest. A postulated assumption may be expressed in words as “\\(X\\) is proportional to \\(Y\\)”, and can be written as the following equation: \\(X = aY\\). Another model may postulate a relationship “\\(X\\) is inversely proportional to the product of \\(Y\\) and \\(Z\\)”, which is expressed as \\(X = a/YZ\\).\nSuppose we want to model the relationship between the height of individuals (\\(H\\)) and their weight (\\(W\\)). Measuring those quantities in some population results in the observation that the weight is proportional to the height, with an additive correction. Then we can write the following mathematical model, based on the empirical evidence: \\(W = a H + c\\)\nIn electricity, Ohm’s law governs the relationship between the flow of charged particles, called current (\\(I\\)), the electric potential (\\(V\\)) and the resistance of a conductor (\\(R\\)). This law states that the current through a conductor is proportional to the potential and inversely proportional to the resistance, and thus can be mathematically formulated: \\[\nI = \\frac{V}{R}\n\\]"
  },
  {
    "objectID": "index.html#variables-and-parameters",
    "href": "index.html#variables-and-parameters",
    "title": "Mathematical Methods for Biology, Part 2",
    "section": "variables and parameters",
    "text": "variables and parameters\nMathematical models formulate relationships between different quantities that can be measured in real systems. There are two different types of quantities in models: variables and parameters. The same measurable quantity can be a variable or a parameter, depending on the role it plays in the model. A variable typically varies, either in time or in space, and the model tracks the changes in its value. On the other hand, a parameter typically usually stays the same for a particular manifestation of the model, e.g. an individual or a specific population. However, parameters can vary from individual to individual, or from population to population.\nIn the height and weight model above, the numbers \\(H\\) are \\(W\\) are the variables, which can change between different individuals. The parameters \\(a\\) and \\(c\\) can either be estimated from data for various subpopulations. Perhaps the values of the parameters are different for young people than for older people, or they are different for those who exercise regularly versus those who do not. Once the parameters have been set, one can predict \\(W\\) given \\(H\\), or vice versa. Of course, since this is a model, it is only an approximation of reality. The deviations of predictions of the model from actual height or weight for an individual may tell us something interesting about the physiology of the individual.\nThere are three quantities in the equation for Ohm’s law, and the distinction between variables and parameters depends on the actual system that is being modeled. In order to distinguish between the two, consider which quantity is set prior to the experiment, and which one may vary over the course of the situation we are trying to model. For instance, if voltage is being applied to a material with constant resistance, and the potential may be varied, then \\(V\\) is the independent variable, \\(I\\) is the dependent variable, and \\(R\\) is a parameter. On the other hand, if the setup uses a variable resistor (known as a potentiometer or pot), and the voltage remains constant, then \\(V\\) is a parameter, while \\(I\\) and \\(R\\) are variables. If both the voltage \\(V\\) and the resistance \\(R\\) can vary at the same time, then all three quantities are variables."
  },
  {
    "objectID": "index.html#units-and-dimensions",
    "href": "index.html#units-and-dimensions",
    "title": "Mathematical Methods for Biology, Part 2",
    "section": "units and dimensions",
    "text": "units and dimensions\nEach variable and parameter has its own dimension, which describes the physical or biological meaning of the quantity. Examples are time, length, number of individuals, or concentration per time. It is important to distinguish the dimension of a quantity from the units of measurement. The same quantity can be measured in different units: length can be in meters or feet, population size can be expressed in individuals or millions of individuals. The value of a quantity depends on the units of measurement, but its essential dimensionality does not.\nThere is a fundamental requirement of mathematical modeling: all the terms in an equation must agree in dimensionality; e.g. time cannot be added to number of sheep, since this sum has no biological meaning. In order to express this rule, we will write the dimension of a quantity \\(X\\) as \\([X]\\). While \\(X\\) refers to a numerical value, \\([X]\\) describes its physical meaning. Then the above statement can be illustrated by the following example: \\[\naX = bY^2 \\; => \\; [aX] = [bY^2]\n\\]\nIn the equation $W = a H + c $ all the terms must have the dimension of weight, because that is the meaning of the left hand side of the equation. Therefore, \\(c\\) has the dimensions of weight as well. \\(H\\) of course has the dimension of length, so this implies that the parameter \\(a\\) has dimensions of weight divided by length. This can be summed up as follows:\n\\[\n[W] = [c] = weight ; \\; [H] = length ; \\; [a] = \\frac{weight }{length}\n\\]\nWhile the dimensions are set by the equation, the units of these quantities can vary. Weight can be expressed in pounds, kilograms, or stones, and length can be represented in inches, meters, or light years.\nThe dimensions of current are defined to be the amount of charge moving per unit of time, and the dimensions of voltage are energy per unit of charge. This allows us to find the dimensions of resistance by the following basic algebra:\n\\[\n[V] = \\frac{energy}{charge} = \\frac{[I]}{[R]} = \\frac{charge/time}{[R]} \\; => \\; [R] = \\frac{charge^2}{energy *time}\n\\]\nElectric potential is measured in volts, and current in amperes. The standard unit of resistance is the Ohm, which is defined as one volt per ampere. But regardless of the choice of units, the dimensions of these quantities remains.\nA quantity may be made dimensionless by expressing it in terms of particular scale. For instance, we can express the height of a person as a fraction of the mean height of the population. A tall person will have height expressed as a number greater than 1, and a short one will have height less than 1. Note that this dimensionless height has no units - they have been divided out by scaling the height by the mean height. In fact, the word dimensionless is somewhat misleading: while such quantities have no scale in the context of the algebraic relationship, a quantity retains its physical significance after rescaling: height expressed as a fraction of some chosen length still represents height. Nevertheless, the accepted term in dimensionless quantity, and we will stick with this convention. Later in the book we will learn how to use the technique of rescaling to simplify and analyze dynamic models."
  },
  {
    "objectID": "PCA.html#motivation-simplifying-complex-data",
    "href": "PCA.html#motivation-simplifying-complex-data",
    "title": "1  Principal Component Analysis",
    "section": "1.1 Motivation: simplifying complex data",
    "text": "1.1 Motivation: simplifying complex data\nSuppose we have a data set with \\(n\\) variables and \\(m\\) observations of each (typically, with \\(n \\gg m\\)), in which the \\(m\\) rows are observations and the \\(n\\) columns are the variables. Each row of this matrix defines a point in the Euclidean space \\(\\mathbb R^n\\). Many biological data sets, e.g. gene expression {numref}fig-micro-array, RNAseq, medical imaging, can contain thousands or more variables, which poses major challenges both for visualization and computational tasks. PCA provides the best representation of such a data set in terms of a smaller set of variables, while capturing as much variance as possible.\n\n\n\nImage of a microarray plate, (http://exploreable.files.wordpress.com/2011/04/array.jpg). Here each dot is a different variable (different gene) and this image in just one set of observations that will be placed into a row of the data matrix.\n\n\nThe intuition behind finding these new collective variables rests on the fact that the original variables have relationships. This is typically measured using covariance, which quantified how much a pair variables tends to move in the same direction (positive covariance) or in opposite directions (negative covariance). If two variables are tighlty coupled, one can replace the two measurements with one, which will describe how much the two of them are deviating in some collective way.\nIt is helpful to think of this geometrically: if the variables are related, the scatterplot of observed data points will have a shape. The goal of PCA is to find directions in the \\(n\\)-dimensional space of observations that best match the shape of the data cloud."
  },
  {
    "objectID": "PCA.html#pca-algorithm",
    "href": "PCA.html#pca-algorithm",
    "title": "1  Principal Component Analysis",
    "section": "1.2 PCA algorithm",
    "text": "1.2 PCA algorithm\nWe start with a data set \\(X\\) in the form of a \\(m\\) by \\(n\\) matrix. The first step is to decide which are the variables and which are the observations. For example, in the case of the microarray experiment, it usually makes sense to consider different genes the variables, and to use principal components to see which genes tend to be expressed together with others.\nThe second step is to compute the variance-covariance matrix of the \\(N\\) variables.\n\n\n\n\n\n\nDefinition\n\n\n\nThe variance-covariance matrix \\(C\\) of a data set \\(X\\) with \\(n\\) variables \\(x_i\\) and \\(m\\) observations is an \\(n\\) by \\(n\\) matrix that contains pairwise variances between all \\(n\\) variables, so that its \\(i\\), \\(j\\) element is:\n\\[C_{i,j} = Cov(X_i,X_j)\\]\n\n\nThe third step is to diagonalize (find the eigenvalues and eigenvectors) of the covariance matrix \\(C\\). The eigenvectors are the principal components of the \\(n\\) variables in the data set, representing linear combinations of the variables that best fit the data. Diagonalizing an \\(n\\) by \\(n\\) matrix results in \\(n\\) eigenvectors, so in order to simplify the description one needs to choose the most significant ones. This is accomplished by choosing a subset of \\(k\\) principal components with the largest eigenvalues. Here are the steps of principal component analysis (PCA):\n\n\n\n\n\n\nPCA algorithm\n\n\n\n\nObtain a dataset as a \\(m\\) by \\(n\\) matrix, with \\(n\\) variables and \\(m\\) observations\nCompute covariances for variable i and variable j, put them in the covariance matrix \\(C\\)\nCompute the eigenvalues and eigenvectors (principal components) of the matrix \\(C\\)\nOrder the principal component by size of eigenvalues from largest to smallest and select a few as the new coordinates"
  },
  {
    "objectID": "PCA.html#optization-by-explained-variance",
    "href": "PCA.html#optization-by-explained-variance",
    "title": "1  Principal Component Analysis",
    "section": "1.3 Optization by explained variance",
    "text": "1.3 Optization by explained variance\nThe reason that we order the PCs by their eigenvalues is that they measure the amount of variance captured by each principal component. In that, they are equivalent to the coefficient of determination \\(r^2\\) in linear regression. The sum of all the eigenvalues is equal to the total variance of all the variables:\n\\[ \\sum_i \\lambda_i = \\sum Var(X_i)\\]\nand the fraction of variance captured by the a principal component is:\n\\[ Var(PC_i) = \\frac{\\lambda_i}{\\sum_i \\lambda_i}\\]\nThe theory behind this rests on some relatively sophisticated linear algebra, in particular what is called the singular value decomposition (SVD) and the Eckart-Young Mirsky theorem. Here is a nice video by Gilbert Strang that explains this:Strang lecture"
  },
  {
    "objectID": "PCA.html#dimensionality-reduction",
    "href": "PCA.html#dimensionality-reduction",
    "title": "1  Principal Component Analysis",
    "section": "1.4 Dimensionality reduction",
    "text": "1.4 Dimensionality reduction\nAfter sorting the principal components and selecting \\(k\\) largest eigenvalues, we are ready to simplify the data. This means that we can express a data set of \\(n\\) variables in terms of the coordinate set of \\(k\\) principal components. In order to express the data set in this new system of coordinates, we compute the projection coefficients for each measurement onto a give principal component. Suppose that \\(Y\\) is a set of measurements of \\(N\\) variables (e.g. genes) and \\(P_i\\) is the \\(i\\)-the principal component. Then the projection coefficient of \\(Y\\) onto \\(P_i\\) is the dot product of the two vectors (both of length \\(N\\)) divided by the squared norm (length) of the PC:\n\\[ c_i = \\frac{\\langle Y, P_i\\rangle}{|| P_i ||^2} \\]\nIf the eigenvectors are normalized prior to the computation (as they are by most computational packages), then the projection coefficient is just the dot product. Then the coefficients can be obtained for all of the measurements in the data set \\(X\\) (\\(m\\) by \\(n\\)) by multiplying it by the matrix \\(P\\) containing the first \\(k\\) eigenvectors (principal components), which has \\(n\\) rows and \\(k\\) columns. The result is an \\(m\\) by \\(k\\) matrix \\(C\\) containing \\(k\\) coefficients for each of the \\(m\\) measurements:\n\\[\nC = D \\times P\n\\]\nHere is the outline for the transformation:\n\n\n\n\n\n\nDimensionality reduction\n\n\n\n\nSubtract the mean of each observation from the data matrix (if it has \\(M\\) observations in rows and \\(N\\) variables as columns, subtract the mean of each row from it)\nCompute the projection coefficients \\(C = D \\times P\\) for each measurement and each of the \\(k\\) principal components\nPlot or otherwise display these coefficients as coordinates in the new vector system of the \\(k\\) PCs. This can be used to cluster or otherwise find patterns in the observations.\n\n\n\nThe entire data set can be expressed in a low-dimensional setting, for instance plotted in the plane of two principal components with coordinates \\((c_{i,1}, c_{i,2})\\) for each data measurement \\(i\\). This is often useful for clustering, or grouping experimental conditions based on the similarity of their principal component representations. Biologists frequently do this with complex data sets, for example grouping different cell lines together by their gene expression profiles."
  },
  {
    "objectID": "PCA.html#optimization-by-explained-variance",
    "href": "PCA.html#optimization-by-explained-variance",
    "title": "1  Principal Component Analysis",
    "section": "1.3 Optimization by explained variance",
    "text": "1.3 Optimization by explained variance\nThe reason that we order the PCs by their eigenvalues is that they measure the amount of variance captured by each principal component. In that, they are equivalent to the coefficient of determination \\(r^2\\) in linear regression. The sum of all the eigenvalues is equal to the total variance of all the variables:\n\\[ \\sum_i \\lambda_i = \\sum Var(X_i)\\]\nand the fraction of variance captured by the a principal component is:\n\\[ Var(PC_i) = \\frac{\\lambda_i}{\\sum_i \\lambda_i}\\]\nThe theory behind this rests on some relatively sophisticated linear algebra, in particular what is called the singular value decomposition (SVD) and the Eckart-Young Mirsky theorem. Here is a nice video by Gilbert Strang that explains this:Strang lecture"
  }
]