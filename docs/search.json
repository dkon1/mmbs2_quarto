[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mathematical Methods for Biology, Part 2",
    "section": "",
    "text": "Preface\nIn this book you will find a collection of mathematical ideas, computational methods, and modeling tools for describing biological systems quantitatively. Biological science, like all natural sciences, is driven by experimental results. As with other sciences, there comes a point when accumulated data needs to be analyzed quantitatively, in order to formulate and test explanatory hypotheses. Biology has reached this stage, thanks to an explosion of data from molecular biology techniques, such as large-scale DNA sequencing, protein structure determination, data on gene regulatory networks, and signaling pathways. Quantitative skills have become necessary for anyone hoping to make sense of biological research.\nMathematical modeling necessarily involves making simplifying assumptions. Reality is generally too complex to be captured in a few equations, and this is especially true for living systems. Simplicity in modeling has at least two virtues: first, simple models can be grasped by our limited minds, and second, it allows for meaningful testing of the assumptions against the evidence. A complex model that fits the data may not provide any insights about how the system works, whereas a simple model which does not fit all the data can indicate where the assumptions break down. We will learn how to construct progressively more sophisticated models, beginning with the ridiculously simple."
  },
  {
    "objectID": "index.html#modeling-assumptions-theoretical-and-empirical",
    "href": "index.html#modeling-assumptions-theoretical-and-empirical",
    "title": "Mathematical Methods for Biology, Part 2",
    "section": "modeling assumptions: theoretical and empirical",
    "text": "modeling assumptions: theoretical and empirical\nA mathematical model postulates a precise relationship between several quantities, attempting to mimic the behavior of a real system. All models rest on a set of assumptions, postulating how various quantities are interrelated. These assumptions generally come from two sources: a scientific theory, or experimental observations. For instance, a model of molecular motion may rest on the assumption that Newton’s laws hold true. On the other hand, the observation that a drug injected into the bloodstream of a mammal is metabolized with an exponential time dependence is empirical. The benefit of models based on well-established theories, sometimes known as “first-principles models”, is that they can be constructed without prior experimental knowledge of a particular system. Newton’s laws apply to all sorts of classical mechanics objects, ranging in size from molecules to planets. Some prefer first-principles models, because they rely on well-established scientific principles, while others will argue that an empirical model more accurately reflects the behavior of the system at hand. From a mathematical standpoint, there is no difference between the two types of models. We will use the same tools to construct and analyze models, regardless of their origin.\nA stated assumption can be written as a mathematical relationship, usually in the form of an equation relating quantities of interest. A postulated assumption may be expressed in words as “\\(X\\) is proportional to \\(Y\\)”, and can be written as the following equation: \\(X = aY\\). Another model may postulate a relationship “\\(X\\) is inversely proportional to the product of \\(Y\\) and \\(Z\\)”, which is expressed as \\(X = a/YZ\\).\nSuppose we want to model the relationship between the height of individuals (\\(H\\)) and their weight (\\(W\\)). Measuring those quantities in some population results in the observation that the weight is proportional to the height, with an additive correction. Then we can write the following mathematical model, based on the empirical evidence: \\(W = a H + c\\)\nIn electricity, Ohm’s law governs the relationship between the flow of charged particles, called current (\\(I\\)), the electric potential (\\(V\\)) and the resistance of a conductor (\\(R\\)). This law states that the current through a conductor is proportional to the potential and inversely proportional to the resistance, and thus can be mathematically formulated: \\[\nI = \\frac{V}{R}\n\\]"
  },
  {
    "objectID": "index.html#variables-and-parameters",
    "href": "index.html#variables-and-parameters",
    "title": "Mathematical Methods for Biology, Part 2",
    "section": "variables and parameters",
    "text": "variables and parameters\nMathematical models formulate relationships between different quantities that can be measured in real systems. There are two different types of quantities in models: variables and parameters. The same measurable quantity can be a variable or a parameter, depending on the role it plays in the model. A variable typically varies, either in time or in space, and the model tracks the changes in its value. On the other hand, a parameter typically usually stays the same for a particular manifestation of the model, e.g. an individual or a specific population. However, parameters can vary from individual to individual, or from population to population.\nIn the height and weight model above, the numbers \\(H\\) are \\(W\\) are the variables, which can change between different individuals. The parameters \\(a\\) and \\(c\\) can either be estimated from data for various subpopulations. Perhaps the values of the parameters are different for young people than for older people, or they are different for those who exercise regularly versus those who do not. Once the parameters have been set, one can predict \\(W\\) given \\(H\\), or vice versa. Of course, since this is a model, it is only an approximation of reality. The deviations of predictions of the model from actual height or weight for an individual may tell us something interesting about the physiology of the individual.\nThere are three quantities in the equation for Ohm’s law, and the distinction between variables and parameters depends on the actual system that is being modeled. In order to distinguish between the two, consider which quantity is set prior to the experiment, and which one may vary over the course of the situation we are trying to model. For instance, if voltage is being applied to a material with constant resistance, and the potential may be varied, then \\(V\\) is the independent variable, \\(I\\) is the dependent variable, and \\(R\\) is a parameter. On the other hand, if the setup uses a variable resistor (known as a potentiometer or pot), and the voltage remains constant, then \\(V\\) is a parameter, while \\(I\\) and \\(R\\) are variables. If both the voltage \\(V\\) and the resistance \\(R\\) can vary at the same time, then all three quantities are variables."
  },
  {
    "objectID": "index.html#units-and-dimensions",
    "href": "index.html#units-and-dimensions",
    "title": "Mathematical Methods for Biology, Part 2",
    "section": "units and dimensions",
    "text": "units and dimensions\nEach variable and parameter has its own dimension, which describes the physical or biological meaning of the quantity. Examples are time, length, number of individuals, or concentration per time. It is important to distinguish the dimension of a quantity from the units of measurement. The same quantity can be measured in different units: length can be in meters or feet, population size can be expressed in individuals or millions of individuals. The value of a quantity depends on the units of measurement, but its essential dimensionality does not.\nThere is a fundamental requirement of mathematical modeling: all the terms in an equation must agree in dimensionality; e.g. time cannot be added to number of sheep, since this sum has no biological meaning. In order to express this rule, we will write the dimension of a quantity \\(X\\) as \\([X]\\). While \\(X\\) refers to a numerical value, \\([X]\\) describes its physical meaning. Then the above statement can be illustrated by the following example: \\[\naX = bY^2 \\; => \\; [aX] = [bY^2]\n\\]\nIn the equation $W = a H + c $ all the terms must have the dimension of weight, because that is the meaning of the left hand side of the equation. Therefore, \\(c\\) has the dimensions of weight as well. \\(H\\) of course has the dimension of length, so this implies that the parameter \\(a\\) has dimensions of weight divided by length. This can be summed up as follows:\n\\[\n[W] = [c] = weight ; \\; [H] = length ; \\; [a] = \\frac{weight }{length}\n\\]\nWhile the dimensions are set by the equation, the units of these quantities can vary. Weight can be expressed in pounds, kilograms, or stones, and length can be represented in inches, meters, or light years.\nThe dimensions of current are defined to be the amount of charge moving per unit of time, and the dimensions of voltage are energy per unit of charge. This allows us to find the dimensions of resistance by the following basic algebra:\n\\[\n[V] = \\frac{energy}{charge} = \\frac{[I]}{[R]} = \\frac{charge/time}{[R]} \\; => \\; [R] = \\frac{charge^2}{energy *time}\n\\]\nElectric potential is measured in volts, and current in amperes. The standard unit of resistance is the Ohm, which is defined as one volt per ampere. But regardless of the choice of units, the dimensions of these quantities remains.\nA quantity may be made dimensionless by expressing it in terms of particular scale. For instance, we can express the height of a person as a fraction of the mean height of the population. A tall person will have height expressed as a number greater than 1, and a short one will have height less than 1. Note that this dimensionless height has no units - they have been divided out by scaling the height by the mean height. In fact, the word dimensionless is somewhat misleading: while such quantities have no scale in the context of the algebraic relationship, a quantity retains its physical significance after rescaling: height expressed as a fraction of some chosen length still represents height. Nevertheless, the accepted term in dimensionless quantity, and we will stick with this convention. Later in the book we will learn how to use the technique of rescaling to simplify and analyze dynamic models."
  },
  {
    "objectID": "PCA.html#motivation-simplifying-complex-data",
    "href": "PCA.html#motivation-simplifying-complex-data",
    "title": "1  Principal Component Analysis",
    "section": "1.1 Motivation: simplifying complex data",
    "text": "1.1 Motivation: simplifying complex data\nSuppose we have a data set with \\(n\\) variables and \\(m\\) observations of each (typically, with \\(n \\gg m\\)), in which the \\(m\\) rows are observations and the \\(n\\) columns are the variables. Each row of this matrix defines a point in the Euclidean space \\(\\mathbb R^n\\). Many biological data sets, e.g. gene expression {numref}fig-micro-array, RNAseq, medical imaging, can contain thousands or more variables, which poses major challenges both for visualization and computational tasks. PCA provides the best representation of such a data set in terms of a smaller set of variables, while capturing as much variance as possible.\n\n\n\nImage of a microarray plate, (http://exploreable.files.wordpress.com/2011/04/array.jpg). Here each dot is a different variable (different gene) and this image in just one set of observations that will be placed into a row of the data matrix.\n\n\nThe intuition behind finding these new collective variables rests on the fact that the original variables have relationships. This is typically measured using covariance, which quantified how much a pair variables tends to move in the same direction (positive covariance) or in opposite directions (negative covariance). If two variables are tighlty coupled, one can replace the two measurements with one, which will describe how much the two of them are deviating in some collective way.\nIt is helpful to think of this geometrically: if the variables are related, the scatterplot of observed data points will have a shape. The goal of PCA is to find directions in the \\(n\\)-dimensional space of observations that best match the shape of the data cloud. This requires tools from linear algebra, specifically the technique of change of basis."
  },
  {
    "objectID": "PCA.html#pca-algorithm",
    "href": "PCA.html#pca-algorithm",
    "title": "1  Principal Component Analysis",
    "section": "1.3 PCA algorithm",
    "text": "1.3 PCA algorithm\nWe start with a data set \\(X\\) in the form of a \\(m\\) by \\(n\\) matrix. The first step is to decide which are the variables and which are the observations. For example, in the case of the microarray experiment, it usually makes sense to consider different genes the variables, and to use principal components to see which genes tend to be expressed together with others.\nThe second step is to compute the variance-covariance matrix of the \\(N\\) variables.\n\n\n\n\n\n\nDefinition\n\n\n\nThe variance-covariance matrix \\(C\\) of a data set \\(X\\) with \\(n\\) variables \\(x_i\\) and \\(m\\) observations is an \\(n\\) by \\(n\\) matrix that contains pairwise variances between all \\(n\\) variables, so that its \\(i\\), \\(j\\) element is:\n\\[C_{i,j} = Cov(X_i,X_j)\\]\n\n\nThe third step is to diagonalize (find the eigenvalues and eigenvectors) of the covariance matrix \\(C\\). The eigenvectors are the principal components of the \\(n\\) variables in the data set, representing linear combinations of the variables that best fit the data. Diagonalizing an \\(n\\) by \\(n\\) matrix results in \\(n\\) eigenvectors, so in order to simplify the description one needs to choose the most significant ones. This is accomplished by choosing a subset of \\(k\\) principal components with the largest eigenvalues. Here are the steps of principal component analysis (PCA):\n\n\n\n\n\n\nPCA algorithm\n\n\n\n\nObtain a dataset as a \\(m\\) by \\(n\\) matrix, with \\(n\\) variables and \\(m\\) observations\nCompute covariances for variable i and variable j, put them in the covariance matrix \\(C\\)\nCompute the eigenvalues and eigenvectors (principal components) of the matrix \\(C\\)\nOrder the principal component by size of eigenvalues from largest to smallest and select a few as the new coordinates"
  },
  {
    "objectID": "PCA.html#optization-by-explained-variance",
    "href": "PCA.html#optization-by-explained-variance",
    "title": "1  Principal Component Analysis",
    "section": "1.3 Optization by explained variance",
    "text": "1.3 Optization by explained variance\nThe reason that we order the PCs by their eigenvalues is that they measure the amount of variance captured by each principal component. In that, they are equivalent to the coefficient of determination \\(r^2\\) in linear regression. The sum of all the eigenvalues is equal to the total variance of all the variables:\n\\[ \\sum_i \\lambda_i = \\sum Var(X_i)\\]\nand the fraction of variance captured by the a principal component is:\n\\[ Var(PC_i) = \\frac{\\lambda_i}{\\sum_i \\lambda_i}\\]\nThe theory behind this rests on some relatively sophisticated linear algebra, in particular what is called the singular value decomposition (SVD) and the Eckart-Young Mirsky theorem. Here is a nice video by Gilbert Strang that explains this:Strang lecture"
  },
  {
    "objectID": "PCA.html#dimensionality-reduction",
    "href": "PCA.html#dimensionality-reduction",
    "title": "1  Principal Component Analysis",
    "section": "1.5 Dimensionality reduction",
    "text": "1.5 Dimensionality reduction\nAfter sorting the principal components and selecting \\(k\\) largest eigenvalues, we are ready to simplify the data. This means that we can express a data set of \\(n\\) variables in terms of the coordinate set of \\(k\\) principal components. In order to express the data set in this new system of coordinates, we compute the projection coefficients for each measurement onto a give principal component. Suppose that \\(Y\\) is a set of measurements of \\(N\\) variables (e.g. genes) and \\(P_i\\) is the \\(i\\)-the principal component. Then the projection coefficient of \\(Y\\) onto \\(P_i\\) is the dot product of the two vectors (both of length \\(N\\)) divided by the squared norm (length) of the PC:\n\\[ c_i = \\frac{\\langle Y, P_i\\rangle}{|| P_i ||^2} \\]\nIf the eigenvectors are normalized prior to the computation (as they are by most computational packages), then the projection coefficient is just the dot product. Then the coefficients can be obtained for all of the measurements in the data set \\(X\\) (\\(m\\) by \\(n\\)) by multiplying it by the matrix \\(P\\) containing the first \\(k\\) eigenvectors (principal components), which has \\(n\\) rows and \\(k\\) columns. The result is an \\(m\\) by \\(k\\) matrix \\(C\\) containing \\(k\\) coefficients for each of the \\(m\\) measurements:\n\\[\nC = D \\times P\n\\]\nHere is the outline for the transformation:\n\n\n\n\n\n\nDimensionality reduction\n\n\n\n\nSubtract the mean of each observation from the data matrix (if it has \\(M\\) observations in rows and \\(N\\) variables as columns, subtract the mean of each row from it)\nCompute the projection coefficients \\(C = D \\times P\\) for each measurement and each of the \\(k\\) principal components\nPlot or otherwise display these coefficients as coordinates in the new vector system of the \\(k\\) PCs. This can be used to cluster or otherwise find patterns in the observations.\n\n\n\nThe entire data set can be expressed in a low-dimensional setting, for instance plotted in the plane of two principal components with coordinates \\((c_{i,1}, c_{i,2})\\) for each data measurement \\(i\\). This is often useful for clustering, or grouping experimental conditions based on the similarity of their principal component representations. Biologists frequently do this with complex data sets, for example grouping different cell lines together by their gene expression profiles."
  },
  {
    "objectID": "PCA.html#optimization-by-explained-variance",
    "href": "PCA.html#optimization-by-explained-variance",
    "title": "1  Principal Component Analysis",
    "section": "1.4 Optimization by explained variance",
    "text": "1.4 Optimization by explained variance\nThe reason that we order the PCs by their eigenvalues is that they measure the amount of variance captured by each principal component. In that, they are equivalent to the coefficient of determination \\(r^2\\) in linear regression. The sum of all the eigenvalues is equal to the total variance of all the variables:\n\\[ \\sum_i \\lambda_i = \\sum Var(X_i)\\]\nand the fraction of variance captured by the a principal component is:\n\\[ Var(PC_i) = \\frac{\\lambda_i}{\\sum_i \\lambda_i}\\]\nThe theory behind this rests on some relatively sophisticated linear algebra, in particular what is called the singular value decomposition (SVD) and the Eckart-Young Mirsky theorem. Here is a nice video by Gilbert Strang that explains this:Strang lecture"
  },
  {
    "objectID": "PCA.html#linearity-and-vector-spaces",
    "href": "PCA.html#linearity-and-vector-spaces",
    "title": "1  Principal Component Analysis",
    "section": "1.2 Linearity and vector spaces",
    "text": "1.2 Linearity and vector spaces\nWe have dealt with linear models in various guises, so now would be a good time to define properly what linearity means. The word comes from the shape of graphs of linear functions of one variable, e.g. \\(f(x) = a x + b\\), but the algebraic meaning rests on the following two general properties:\n\n\n\n\n\n\nDefinition\n\n\n\nA linear transformation or linear operator is a mapping \\(L\\) between two sets of vectors with the following properties:\n\n(scalar multiplication) \\(L(c \\vec v) = c L(\\vec v)\\); where \\(c\\) is a scalar and \\(\\vec v\\) is a vector;\n(additive) \\(L(\\vec v_1 + \\vec v_2) = L(\\vec v_1) + L(\\vec v_2)\\); where \\(\\vec v_1\\) and \\(\\vec v_2\\) are vectors.\n\n\n\nHere we have two types of objects: vectors and transformations/operators that act on those vectors. The basic example of this are vectors and matrices, because a matrix multiplied by a vector (on the right) results another vector, provided the number of columns in the matrix is the same as the number of rows in the vector. This can be interpreted as the matrix transforming the vector \\(\\vec v\\) into another one: $ A v = u$.\nExample: Let us multiply the following matrix and vector (specially chosen to make a point):\n\n\n\n\nMat = np.array([[2, 1],[2, 3]])\nvec1 = np.array([1, -1])\nvec2 = Mat@vec1\nprint(vec1)\n\n[ 1 -1]\n\nprint(vec2)\n\n[ 1 -1]\n\n\nWe see that this particular vector \\((1,-1)\\) is unchanged when multiplied by this matrix, or we can say that the matrix multiplication is equivalent to multiplication by 1. Here is another such vector for the same matrix:\n\nvec1 = np.array([1, 2])\nvec2 = Mat@vec1\nprint(vec1)\n\n[1 2]\n\nprint(vec2)\n\n[4 8]\n\n\nIn this case, the vector is changed, but only by multiplication by a constant (4). Thus the geometric direction of the vector remained unchanged.\nThe notion of linearity leads to the important idea of combining different vectors:\n\n\n\n\n\n\nDefinition\n\n\n\nA linear combination of \\(n\\) vectors \\(\\{ \\vec v_i \\}\\) is a weighted sum of these vectors with any real numbers \\(\\{a_i\\}\\): \\[ a_1 \\vec v_1+ a_2 \\vec v_2... + a_n \\vec v_n\\]\n\n\nLinear combinations arise naturally from the notion of linearity, combining the additive property and the scalar multiplication property. Speaking intuitively, a linear combination of vectors produces a new vector that is related to the original set. Linear combinations give a simple way of generating new vectors, and thus invite the following definition for a collection of vectors closed under linear combinations:\n\n\n\n\n\n\nDefinition\n\n\n\nA vector space is a collection of vectors such that a linear combination of any \\(n\\) vectors is contained in the vector space.\n\n\nThe most common examples are the spaces of all real-valued vectors of dimension \\(n\\), which are denoted by \\(\\mathbb{R}^n\\). For instance, \\(\\mathbb{R}^2\\) (pronounced “r two”) is the vector space of two dimensional real-valued vectors such as \\((1,3)\\) and \\((\\pi, -\\sqrt{17})\\); similarly, \\(\\mathbb{R}^3\\) is the vector space consisting of three dimensional real-valued vectors such as \\((0.1,0,-5.6)\\). You can convince yourself, by taking linear combinations of vectors, that these vector spaces contain all the points in the usual Euclidean plane and three-dimensional space. The real number line can also be thought of as the vector space \\(\\mathbb{R}^1\\).\n\n1.2.1 Linear independence and basis vectors\nHow can we describe a vector space without trying to list all of its elements? We know that one can generate an element by taking linear combinations of vectors. It turns out that it is possible to generate (or “span”) a vector space by taking linear combinations of a subset of its vectors. The challenge is to find a minimal subset of subset that is not redundant. In order to do this, we first introduce a new concept:\n\n\n\n\n\n\nDefinition\n\n\n\nA set of vectors \\(\\{ \\vec v_i \\}\\) is called linearly independent if the only linear combination involving them that equals the zero vector is if all the coefficients are zero. ( \\(a_1 \\vec v_1 + a_2 \\vec v_2 + ... + a_n \\vec v_n = 0\\) only if \\(a_i = 0\\) for all \\(i\\).)\n\n\nIn the familiar Euclidean spaces, e.g. \\(\\mathbb{R}^2\\), linear independence has a geometric meaning: two vectors are linearly independent if the segments from the origin to the endpoint do not lie on the same line. But it can be shown that any set of three vectors in the plane is linearly dependent, because there are only two dimensions in the vector space. This brings us to the key definition of this section:\n\n\n\n\n\n\nDefinition\n\n\n\nA basis of a vector space is a linearly independent set of vectors that generate (or span) the vector space. The number of vectors (cardinality) in such a set is called the dimension of the vector space.\n\n\nA vector space generally has many possible bases, as illustrated in figure. In the case of \\(\\mathbb{R}^2\\), the usual (canonical) basis set is \\(\\{(1,0); (0,1)\\}\\) which obviously generates any point on the plane and is linearly independent. But any two linearly independent vectors can generate any vector in the plane.\nExample: The vector \\(\\vec r = (2,1)\\) can be represented as a linear combination of the two canonical vectors: \\(\\vec r = 2\\times (1,0)+1\\times (0,1)\\). Let us choose another basis set, say \\(\\{(1,1); (-1,1)\\}\\) (this is the canonical basis vectors rotated by \\(\\pi/2\\).) The same vector can be represented by a linear combination of these two vectors, with coefficients \\(1.5\\) and \\(-0.5\\): \\(\\vec r = 1.5\\times (1,1) - 0.5 \\times (-1,1)\\). If we call the first basis \\(C\\) for canonical and the second basis \\(D\\) for different, we can write the same vector using different sets of coordinates for each basis:\n\\[\n\\vec r_{C} = (2,1); \\; \\vec r_D = (1.5, -0.5)\n\\]\n\n\n1.2.2 Projections and changes of basis\nThe representation of an arbitrary vector (point) in a vector space as a linear combination of a given basis set is called the decomposition of the point in terms of the basis, which gives the coordinates for the vector in terms of each basis vector. The decomposition of a point in terms of a particular basis is very useful in high-dimensional spaces, where a clever choice of a basis can allow a description of a set of points (such as a data set) in terms of contributions of only a few basis vectors, if the data set primarily extends only in a few dimensions.\nTo obtain the coefficients of the basis vectors in a decomposition of a vector \\(\\vec r\\), we need to perform what is termed a projection of the vector onto the basis vectors. Think of shining a light perpendicular to the basis vector, and measuring the length of the shadow cast by the vector \\(\\vec r\\) onto \\(\\vec v_i\\). If the vectors are parallel, the shadow is equal to the length of \\(\\vec r\\); if they are orthogonal, the shadow is nonexistent. To find the length of the shadow, use the inner product of \\(\\vec r\\) and \\(\\vec v\\), which as you recall corresponds to the cosine of the angle between the two vectors multiplied by their norms: $r, v=rv() $. We do not care about the length of the vector \\(\\vec v\\) we are projecting onto, thus we divide the inner product by the square norm of \\(\\vec v\\), and then multiply the vector \\(\\vec v\\) by this projection coefficient:\n\\[\nProj(\\vec r ; \\vec v) = \\frac{ \\langle \\vec r , \\vec v \\rangle  } {\\langle \\vec v , \\vec v \\rangle } \\vec v = \\frac{ \\langle \\vec r ,  \\vec v \\rangle  } {\\vert \\vec v \\vert^2} \\vec v= \\frac{  \\vert\\vec r\\vert \\cos(\\theta) } {\\vert \\vec v \\vert}\\vec v\n\\]\nThis formula gives the projection of the vector \\(\\vec r\\) onto \\(\\vec v\\), the result is a new vector in the direction of \\(\\vec v\\), with the scalar coefficient \\(a = \\ \\langle \\vec r , \\vec v \\rangle /\\vert \\vec v \\vert^2\\).\nExample: Here is how one might calculate the projection of the point \\((2,1)\\) onto the basis set \\(\\{(1,1); (-1,1)\\}\\):\n\nv1 <- c(1, 1)\nv2 <- c(-1, 1)\nu <- c(2, 1)\nProjMat <- matrix(cbind(v1, v2), \n                  byrow = T, nrow = 2)\nprint(ProjMat)\n\n     [,1] [,2]\n[1,]    1    1\n[2,]   -1    1\n\nProjMat %*% u\n\n     [,1]\n[1,]    3\n[2,]   -1\n\n\nThis is not quite right: the projection coefficients are off by a factor of two compared to the correct values in the example above. This is because we have neglected to normalize the basis vectors, so we should modify the script as follows:\n\nv1 <- c(1, 1)\nv1 <- v1 / (sum(v1^2))\nv2 <- c(-1, 1)\nv2 <- v2 / (sum(v2^2))\nu <- c(2, 1)\nProjMat <- matrix(cbind(v1, v2), \n                  byrow = T, nrow = 2)\nprint(ProjMat)\n\n     [,1] [,2]\n[1,]  0.5  0.5\n[2,] -0.5  0.5\n\nprint(ProjMat %*% u)\n\n     [,1]\n[1,]  1.5\n[2,] -0.5\n\n\nThis is an example of how to convert a vector/point from representation in one basis set to another. The new basis vectors, expressed in the original basis set, are arranged in a matrix by row, scaled by their norm squared, and multiplied by the vector that one wants to express in the new basis. The resulting vector contains the coordinates in the new basis."
  }
]